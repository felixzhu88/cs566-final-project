{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c3db91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "from imutils import face_utils\n",
    "from EAR import eye_aspect_ratio\n",
    "from MAR import mouth_aspect_ratio\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('./dlib_shape_predictor/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "EYE_AR_THRESH = 0.25\n",
    "MOUTH_AR_THRESH = 0.79\n",
    "HEADPOSE_THRESH = 25   \n",
    "\n",
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "(mStart, mEnd) = (49, 68)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c2d206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_drowsiness(frame):\n",
    "    \"\"\"\n",
    "    ËæìÂÖ•: BGR ÂõæÂÉè (cv2.imreadËØªÂèñÁöÑ)\n",
    "    ËæìÂá∫: \"Drowsy\" Êàñ \"Non Drowsy\"\n",
    "    \"\"\"\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = detector(gray, 0)\n",
    "\n",
    "    if len(rects) == 0:\n",
    "        return \"Non Drowsy\"   \n",
    "\n",
    "    rect = rects[0]\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "    # EAR\n",
    "    leftEye = shape[lStart:lEnd]\n",
    "    rightEye = shape[rStart:rEnd]\n",
    "    ear = (eye_aspect_ratio(leftEye) + eye_aspect_ratio(rightEye)) / 2.0\n",
    "\n",
    "    # MAR\n",
    "    mouth = shape[mStart:mEnd]\n",
    "    mar = mouth_aspect_ratio(mouth)\n",
    "\n",
    "    size = gray.shape\n",
    "\n",
    "    if ear < EYE_AR_THRESH:\n",
    "        return \"Drowsy\"\n",
    "    if mar > MOUTH_AR_THRESH:\n",
    "        return \"Drowsy\"\n",
    "\n",
    "    return \"Non Drowsy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efe9753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Non Drowsy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 973/973 [00:05<00:00, 191.54it/s]\n",
      "Processing Drowsy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1118/1118 [00:05<00:00, 205.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úî Accuracy: 0.49115255858440937\n",
      "‚úî F1 Score: 0.11333333333333333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Non Drowsy       0.48      0.99      0.64       973\n",
      "      Drowsy       0.83      0.06      0.11      1118\n",
      "\n",
      "    accuracy                           0.49      2091\n",
      "   macro avg       0.65      0.52      0.38      2091\n",
      "weighted avg       0.67      0.49      0.36      2091\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 959   14]\n",
      " [1050   68]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_dir = \"../data/splitted_Data/test\"\n",
    "classes = [\"Non Drowsy\", \"Drowsy\"]   \n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for cls in classes:\n",
    "    folder = os.path.join(test_dir, cls)\n",
    "    label = cls  \n",
    "\n",
    "    for img_name in tqdm(os.listdir(folder), desc=f\"Processing {cls}\"):\n",
    "        if not img_name.endswith(\".png\"):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(folder, img_name)\n",
    "        frame = cv2.imread(img_path)\n",
    "\n",
    "        pred = predict_drowsiness(frame)\n",
    "\n",
    "        y_true.append(label)\n",
    "        y_pred.append(pred)\n",
    "\n",
    " \n",
    "label_map = {\"Non Drowsy\": 0, \"Drowsy\": 1}\n",
    "y_true = [label_map[l] for l in y_true]\n",
    "y_pred = [label_map[p] for p in y_pred]\n",
    "\n",
    " \n",
    "print(\"\\n‚úî Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(\"‚úî F1 Score:\", f1_score(y_true, y_pred, average=\"binary\"))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Non Drowsy\", \"Drowsy\"]))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b4a340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Non Drowsy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 973/973 [00:05<00:00, 192.49it/s]\n",
      "Processing Drowsy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1118/1118 [00:05<00:00, 195.55it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'None Drowsy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m label_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon Drowsy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDrowsy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[1;32m     31\u001b[0m y_true \u001b[38;5;241m=\u001b[39m [label_map[l] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m y_true]\n\u001b[0;32m---> 32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m [label_map[p] \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m y_pred]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# üéØ ËæìÂá∫ÊåáÊ†á\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úî Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_true, y_pred))\n",
      "Cell \u001b[0;32mIn[26], line 32\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m label_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon Drowsy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDrowsy\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[1;32m     31\u001b[0m y_true \u001b[38;5;241m=\u001b[39m [label_map[l] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m y_true]\n\u001b[0;32m---> 32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m [\u001b[43mlabel_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m y_pred]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# üéØ ËæìÂá∫ÊåáÊ†á\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m‚úî Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(y_true, y_pred))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'None Drowsy'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ccea1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f8e62b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "from imutils.video import VideoStream\n",
    "from imutils import face_utils\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import dlib\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "from EAR import eye_aspect_ratio\n",
    "from MAR import mouth_aspect_ratio\n",
    "def process_video_withDetails(input_video_path, output_video_path=\"output.mp4\"):\n",
    "    print(\"[INFO] loading facial landmark predictor...\")\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\n",
    "        './dlib_shape_predictor/shape_predictor_68_face_landmarks.dat'\n",
    "    )\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"‚ùå Failed to open input video!\")\n",
    "        return\n",
    "\n",
    "    # video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    print(f\"[INFO] Video: {width}x{height}, {fps:.2f} FPS\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'avc1')\n",
    "    writer = cv2.VideoWriter(\"output.avi\", fourcc, fps, (width, height))\n",
    "\n",
    "    # EAR / MAR related\n",
    "    EYE_AR_THRESH = 0.25\n",
    "    MOUTH_AR_THRESH = 0.79\n",
    "    EYE_AR_CONSEC_FRAMES = 3\n",
    "    COUNTER = 0\n",
    "\n",
    "    (lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "    (mStart, mEnd) = (49, 68)\n",
    "\n",
    "    # head pose ref points (init won't be used but updated each frame)\n",
    "    image_points = np.zeros((6, 2), dtype=\"double\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = imutils.resize(frame, width=1024)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        rects = detector(gray, 0)\n",
    "\n",
    "        if len(rects) > 0:\n",
    "            cv2.putText(frame, f\"{len(rects)} face(s) found\", (10, 50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,0,255), 2)\n",
    "\n",
    "        for rect in rects:\n",
    "            (bX, bY, bW, bH) = face_utils.rect_to_bb(rect)\n",
    "            cv2.rectangle(frame, (bX, bY), (bX+bW,bY+bH),(0,255,0),1)\n",
    "\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            # EYE\n",
    "            leftEye = shape[lStart:lEnd]\n",
    "            rightEye = shape[rStart:rEnd]\n",
    "            ear = (eye_aspect_ratio(leftEye) + eye_aspect_ratio(rightEye)) / 2.0\n",
    "            leftHull = cv2.convexHull(leftEye)\n",
    "            rightHull = cv2.convexHull(rightEye)\n",
    "            cv2.drawContours(frame,[leftHull],-1,(0,255,0),1)\n",
    "            cv2.drawContours(frame,[rightHull],-1,(0,255,0),1)\n",
    "\n",
    "            if ear < EYE_AR_THRESH:\n",
    "                COUNTER += 1\n",
    "                if COUNTER >= EYE_AR_CONSEC_FRAMES:\n",
    "                    STATUS = \"Drowsy\"\n",
    "                    cv2.putText(frame, \"Eyes Closed!\", (300,100),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1.5,(0,0,255),2)\n",
    "            else:\n",
    "                STATUS = \"Non Drowsy\"\n",
    "                COUNTER = 0\n",
    "\n",
    "            # MOUTH\n",
    "            mouth = shape[mStart:mEnd]\n",
    "            mar = mouth_aspect_ratio(mouth)\n",
    "            mouthHull = cv2.convexHull(mouth)\n",
    "            cv2.drawContours(frame,[mouthHull],-1,(0,255,0),1)\n",
    "            cv2.putText(frame, f\"EAR: {ear:.2f}\", (650,50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,1.5,(0,0,255),2)\n",
    "            cv2.putText(frame, f\"MAR: {mar:.2f}\", (400,50),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,1.5,(0,0,255),2)\n",
    "            if mar > MOUTH_AR_THRESH:\n",
    "                STATUS = \"Drowsy\"\n",
    "                cv2.putText(frame, \"Yawning!\", (800, 100),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,1.5,(0,0,255),2)\n",
    "                \n",
    "            # Draw Overall Status\n",
    "            if STATUS == \"Drowsy\":\n",
    "                color = (0, 0, 255)\n",
    "                text = \"DROWSY!\"\n",
    "            else:\n",
    "                color = (0, 255, 0)\n",
    "                text = \"Non Drowsy\"\n",
    "\n",
    "            # Landmark Visualization\n",
    "            for (i,(x,y)) in enumerate(shape):\n",
    "                cv2.circle(frame,(x,y),1,(0,0,255),-1)\n",
    "            cv2.putText(frame, text, (50, height - 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
    "        writer.write(frame)\n",
    "        cv2.imshow(\"Output\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    writer.release()\n",
    "\n",
    "    # Prevent hanging window on macOS\n",
    "    for i in range(10):\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"üé¨ Done! Output saved => {output_video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c3843499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading facial landmark predictor...\n",
      "[INFO] Video: 544x960, 30.00 FPS\n",
      "üé¨ Done! Output saved => result.mp4\n"
     ]
    }
   ],
   "source": [
    "process_video_withDetails(\"../self-uploaded/drowsy.mp4\", \"result.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b0f65e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.16)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
